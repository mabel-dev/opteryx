
- Put the stats into the logical plan, have a "bind stats" step. bind min/max/null count, for each file, record the range and counts.
    - we can then start to build a cost-based planner, where we can estimate selectivity
    - we can better write correlated filters
    - we can estimate rows after a join

- If we have stats for all of the files and it's a simple agg (count/min/max) solve it from the stats (this will rarely be useful IRL)

- Benchmark the Iceberg reader vs our reader - if we're faster bypass the iceberg reader

- Phase out numpy - use arrow as the preferred format in cython

- Rewrite the expression engine in cython
    - probably use roaring bitmaps
    - compile common expressions (e.g. WHERE str_column = 'value' AND int_column = value) to a function [e.g. str_eq_and_int_eq(str_col, str_val, int_col, int_val)], this then executes with less branching. we can build, complile and cache these - probably dynamically write and compile the cython.

- Rewrite GROUP BY to avoid Arrow's GROUP BY

- bind the statistics to the plan
    - put the list of files to read into the plan
    - there should be a summary (min/max/count etc) for the dataset
    - each file should have it's statistics added if they're in the cache, otherwise just the first and the dataset with an "estimated" flag
    - we can then respond to min/max/count from statistics in the optimizer if we don't have the "estimated" flag set. 

- Start capturing streaming data and writing to an Iceberg dataset (smart home events, Mastodon)
    ✅ Mastodon
    •⁠  ⁠Open API, supports streaming via WebSockets and Server-Sent Events (SSE)
    •⁠  ⁠You can stream public timelines, hashtags, or specific accounts
    •⁠  ⁠Each server (instance) exposes its own API, but most conform to the Mastodon API spec
    •⁠  ⁠Example endpoint:  
    ⁠ wss://mastodon.social/api/v1/streaming/public ⁠

- See if we can use Distogram to estimate selectivity for our cost-based-planner. 
    - maybe rewrite key parts of distogram in cython
    - maybe rewrite the brute-force cost-estimator in cython

- fix the typed tweet data for the CAST tests timestamps... we used UNIXTIME to initially create, which is 'seconds' not 'microseconds'
